{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import sys\n",
    "import traceback\n",
    "from rich.console import Console\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # Import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score  # Import cross_val_score\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "import yarppg\n",
    "import cv2 as cv\n",
    "from deepface import DeepFace\n",
    "from collections import Counter\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from aniemore.recognizers.voice import VoiceRecognizer\n",
    "from aniemore.recognizers.multimodal import VoiceTextRecognizer\n",
    "from aniemore.recognizers.multimodal import MultiModalRecognizer\n",
    "from aniemore.utils.speech2text import SmallSpeech2Text\n",
    "from aniemore.models import HuggingFaceModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "emotion_counter = Counter()\n",
    "\n",
    "# # Для распознавания интонации\n",
    "# model = HuggingFaceModel.Voice.WavLM\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# vr = VoiceRecognizer(model=model, device=device)\n",
    "\n",
    "# Мультимодальный распознаватор\n",
    "model = HuggingFaceModel.MultiModal.WavLMBertFusion\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mr = MultiModalRecognizer(model=model, s2t_model=SmallSpeech2Text(), device=device)\n",
    "\n",
    "# настраиваем распознаватор пульса\n",
    "# fps = 30 #yarppg.get_video_fps(filename)\n",
    "# filter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\")\n",
    "# livefilter = yarppg.digital_filter.make_digital_filter(filter_cfg)\n",
    "# processor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter)\n",
    "# rppg = yarppg.Rppg(\n",
    "#     processor=processor,\n",
    "#     hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4),\n",
    "# )\n",
    "\n",
    "# сканируем папку эмоции и дергаем названия эмоций - вложенных папок\n",
    "data_dir = 'emotions'\n",
    "emotions = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "console.print(\"Найдены эмоции:\", emotions)\n",
    "\n",
    "# смотрим скок файлов во вложенных папках\n",
    "total = 0\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    total += len(files)\n",
    "console.print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем прогрессбар\n",
    "bar = tqdm(total= total)\n",
    "\n",
    "# создаем списки для фич и соответствующих им эмоций\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# залезаем по очереди в каждую папку с эмоцией\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(data_dir, emotion)# папка с эмоциями\n",
    "    # получаем список файлов-видосов в ней\n",
    "    files = [f for f in os.listdir(emotion_dir) if f.endswith('.mp4')]\n",
    "    # и перебираем по очереди все файлы\n",
    "    for file in files:\n",
    "        # console.print(f\"Обрабатываем файл: {file} Эмоция: {emotion}\")\n",
    "        # полный путь к файлу\n",
    "        file_path = os.path.join(emotion_dir, file)\n",
    "        # на всякий случай оборачиваем в трай-кэтч, вдруг что-то пойдет не так\n",
    "        # чтоб программа не посыпалась\n",
    "        try:\n",
    "            ##############################################################################\n",
    "            # начинаем с разпознавания эмоций по видео с помощью дипфейса\n",
    "            # открываем видеофайл\n",
    "            cap = cv.VideoCapture(file_path)\n",
    "\n",
    "            # очищаем-обнуляем все списки-переменные\n",
    "            emotion_counter.clear()\n",
    "            statka = [] #очищаем массив\n",
    "            new_statka = []\n",
    "            final_statka = []\n",
    "            cnt = 0 # счетчик кадров\n",
    "            # это выпилить, можно просто массив засунуть\n",
    "            v_angry = v_disgust = v_fear = v_happy = v_sad = v_surprise = v_neutral = []\n",
    "\n",
    "            # пока файл открыт - работаем с кадрами\n",
    "            while cap.isOpened():\n",
    "                pass\n",
    "                ret, frame = cap.read()\n",
    "                # если вдруг чтение кадра выдало ошибку - брейкаем\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # берем каждый 30-цатый кадр\n",
    "                if cnt % 30 == 0:\n",
    "                    # # находим на картинке лицо с помощью каскада хаара\n",
    "                    # faceCascade= cv.CascadeClassifier(cv.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "                    # imgGray = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)                              \n",
    "                    # faces = faceCascade.detectMultiScale(imgGray,1.1,7)\n",
    "                    # for (xx,yy,ww,hh) in faces:\n",
    "\n",
    "                    #     # и оставляем только лицо, так дипфейс будет работать быстрее\n",
    "                    #     frame = frame [yy:yy+hh, xx:xx+ww]\n",
    "\n",
    "                    # анализируем лицо дипфейсом и получаем главную эмоцию кадра и проценты по всем эмоциям\n",
    "                    result = DeepFace.analyze(frame, actions=['emotion'], \n",
    "                                            enforce_detection=False,  \n",
    "                                            # detector_backend = 'retinaface',\n",
    "                                            # detector_backend = 'mtcnn', \n",
    "                                            # detector_backend ='opencv',\n",
    "                                            detector_backend = 'ssd'\n",
    "                                            )\n",
    "\n",
    "                    # сохраняем в список статка главную эмоцию кадра + проценты по эмоциям \n",
    "                    statka.append([result[0][\"dominant_emotion\"], result[0][\"emotion\"]])\n",
    "                    # добавляем главную эмоцию кадра в высчитыватель главной эмоции видер\n",
    "                    emotion_counter.update([result[0][\"dominant_emotion\"]])\n",
    "                # счетчик кадра +1\n",
    "                cnt += 1\n",
    "\n",
    "                if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            # после завершения обработки видоса, высчитываем главную эмоцию видоса\n",
    "            # можно это не делать и взять название эмоции из папки\n",
    "            # но т.к. датасет не идеален - в некоторых видео не обнаруживается эта эмоция :(\n",
    "            most_common_emotion, _ = emotion_counter.most_common(1)[0]\n",
    "            # конвертируем список в нумпай-список\n",
    "            statka = np.array(statka)\n",
    "\n",
    "            # оставляем только строки с доминантной эмоцией \n",
    "            # и в список нью_статка записываем только значения (вероятности) всех эмоций\n",
    "            # их названия мы и так знаем, а позиции их статичны\n",
    "            \n",
    "            # тут можно заменить мост_коммон_эмошн на эмошн-название папки\n",
    "            # но как я написал раньше - в некоторых видео вообще не совпадает главная эмоция с названием папки\n",
    "            # и список нью статка получается пустым, а так хоть что-то\n",
    "\n",
    "            # хотя можно попробовать сделать модель и без этих файлов и сравнить результаты\n",
    "            new_statka = [list(x[1].values()) for x in statka if x[0] == most_common_emotion]\n",
    "            # конвертируем список в нумпай-список для нумпай-операций\n",
    "            new_statka = np.array(new_statka)\n",
    "\n",
    "            # находим среднее арифметическое столбцов (аксис = 0)\n",
    "            final_statka = np.mean(new_statka, axis=0)\n",
    "            # превращаем экспон. значения в обычные дроби\n",
    "            final_statka = [float(x) for x in final_statka]\n",
    "\n",
    "            v_angry     = [final_statka[0]]\n",
    "            v_disgust   = [final_statka[1]]\n",
    "            v_fear      = [final_statka[2]]\n",
    "            v_happy     = [final_statka[3]]\n",
    "            v_sad       = [final_statka[4]]\n",
    "            v_surprise  = [final_statka[5]]\n",
    "            v_neutral   = [final_statka[6]]\n",
    "\n",
    "            # освобождаем-очищаем\n",
    "            cap.release()\n",
    "            # cv.destroyAllWindows()\n",
    "\n",
    "            ###############################################################################\n",
    "            # анализируем аудио и текст с помощью аниморе\n",
    "\n",
    "            audio_features_vals = []\n",
    "            a_angry = a_disgust = a_fear = a_happy = a_sad = a_surprise = a_neutral = []\n",
    "\n",
    "            # дергаем звук из видео и сохраняем его в темп.вав\n",
    "            mp4_version = AudioSegment.from_file(file_path, \"mp4\")\n",
    "            mp4_version.export(\"temp.wav\", format=\"wav\")\n",
    "\n",
    "            # запускаем распознаватор эмоций по аудио\n",
    "            # audio_features = vr.recognize(\"temp.wav\", return_single_label=False)\n",
    "\n",
    "            # запускаем мультимодальный распознаватор эмоций\n",
    "            audio_features = mr.recognize(\"temp.wav\", return_single_label=False)\n",
    "\n",
    "            # получаем статистику - сколько % каждой эмоции в аудио\n",
    "            audio_features_vals = list(audio_features.values()) \n",
    "\n",
    "            a_angry     = [audio_features_vals[0] * 100]\n",
    "            a_disgust   = [audio_features_vals[1] * 100]\n",
    "            a_fear      = [audio_features_vals[3] * 100]\n",
    "            a_happy     = [audio_features_vals[4] * 100]\n",
    "            a_sad       = [audio_features_vals[6] * 100]\n",
    "            a_surprise  = [audio_features_vals[2] * 100]\n",
    "            a_neutral   = [audio_features_vals[5] * 100]\n",
    "\n",
    "            #####################################################################################\n",
    "            \n",
    "            # # делаем фиче-вектор, складываем все списки с результатами в один большой результат\n",
    "            features = np.concatenate((\n",
    "                # сюда запихнуть все данные\n",
    "                v_angry, v_disgust, v_fear, v_happy, v_sad, v_surprise, v_neutral\n",
    "                # потом запихнуть аудио\n",
    "                , a_angry, a_disgust, a_fear, a_happy, a_sad, a_surprise, a_neutral\n",
    "                # и пульс\n",
    "            ))\n",
    "\n",
    "            # console.print (features)\n",
    "\n",
    "            # # Запихиваем в списки, в х - вектор, в у - название эмоции для пос-го обучения\n",
    "            X.append(features)  \n",
    "            y.append(emotion) \n",
    "\n",
    "        # ну мало ли, вдруг что-то отвалится\n",
    "        except AttributeError:\n",
    "            pass\n",
    "            traceback.print_exc()\n",
    "        except Exception as e:\n",
    "            # AttributeError\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(exc_type, fname, exc_tb.tb_lineno)\n",
    "            # print(f\"Error loading {file}: {e.with_traceback}\")   \n",
    "\n",
    "        bar.update(1) # увеличиваем на единицу\n",
    "bar.close() # Заканчиваем работу бара\n",
    "\n",
    "# конвертируем в нампай-списки\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "console.print(f\"Число фиче-векторов: {len(X)}\")\n",
    "console.print(f\"Число эмоций: {len(y)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут начинается обучение\n",
    "\n",
    "# делим списки на трейн и тест 80 к 20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Нормализация фич с помощью StandardScaler\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)  # Fit to the training data and transform\n",
    "X_test = scaler.transform(X_test)  # Only transform the test data\n",
    "\n",
    "# Кодирование эмоций\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # конвертируем названия эмоций в числовые значения\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже попробуем 4 классификатора, посмотрим, какой из них точнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод опорных векторов\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(decision_function_shape=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# случайный лес\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# многослойный персептрон\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model_params = {\n",
    "        \"alpha\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"epsilon\": 1e-08,\n",
    "        \"hidden_layer_sizes\": (300,),\n",
    "        \"learning_rate\": \"adaptive\",\n",
    "        \"max_iter\": 500\n",
    "    }\n",
    "   \n",
    "# model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\n",
    "model = MLPClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# деревья с градиентным бустингом\n",
    "# будем использовать этот классификатор как наиболее быстрый и точный\n",
    "# model = XGBClassifier(eval_metric='mlogloss',objective=\"binary:logistic\")\n",
    "model = XGBClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пробуем сделать 5-шаговую кросс-валидацию\n",
    "cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
    "console.print(f'Cross-Validation Scores: {cv_scores}')\n",
    "console.print(f'Mean Cross-Validation Score: {np.mean(cv_scores):.2f}')\n",
    "\n",
    "# обучаем модель\n",
    "model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем предсказания на тестовом наборе\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# получаем названия эмоций, декодируем у-результаты\n",
    "y_pred_decoded = label_encoder.inverse_transform(y_pred)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Высчитываем точность\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "console.print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
    "console.print(f'Precision: {precision:.2f}')\n",
    "\n",
    "recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
    "console.print(f'Recall: {recall:.2f}')\n",
    "\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "console.print(f'F1: {f1:.2f}')\n",
    "\n",
    "# выводим отчет о классификации\n",
    "console.print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# показываем на экране Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_encoded, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.ylabel('Реальные')\n",
    "plt.xlabel('Предсказанные')\n",
    "plt.title('Confusion Matrix: Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# # показываем на экране Plot Feature Importance\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# plot_importance(model, max_num_features=10, importance_type='weight')\n",
    "# plt.title('Feature Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем модель в файл\n",
    "filename = 'my_model.sav'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка модели из файла\n",
    "# with open(filename, 'rb') as file:\n",
    "#     pickle_model = pickle.load(file)\n",
    "\n",
    "# # Calculate the accuracy score and predict target values\n",
    "# score = pickle_model.score(X_test, y_test_encoded)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
